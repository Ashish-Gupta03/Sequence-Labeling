{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn_crfsuite, re\n",
    "import numpy as np\n",
    "import importlib, os\n",
    "import logging, math\n",
    "import json, nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens, gram_len=2):\n",
    "    out = []\n",
    "    for i in range(len(tokens)-gram_len+1):\n",
    "        new_token = ' '.join(tokens[i:(i+gram_len)])\n",
    "        out.append(new_token.strip())\n",
    "    return out\n",
    "\n",
    "def clean_tokens(tokens, to_replace='[^\\w ]+'):\n",
    "    tokens = [re.sub(to_replace, ' ', token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def tokenize(mystr, is_char=False):\n",
    "    return mystr.split() if is_char is False else list(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentence, min_ngram=1, max_ngram=1, to_replace='[^\\w ]+', is_char=False):\n",
    "    # print(\"============================\")\n",
    "    # print(\"sentence before \",sentence)\n",
    "    if is_char is False:\n",
    "        sentence = re.sub('<[^<]+?>', ' ', sentence)\n",
    "        sentence = re.sub(to_replace, '', sentence)\n",
    "    # print(\"sentence after \",sentence)\n",
    "    tokens = clean_tokens(tokenize(sentence), to_replace) if is_char is False else tokenize(sentence, is_char)\n",
    "    # print(\"tokens before \",tokens)\n",
    "    tokens = [token.strip() for token in tokens] if is_char is False else [token for token in tokens]\n",
    "    # print(\"tokens after \",tokens)\n",
    "\n",
    "    n_grams = []\n",
    "    for gram_len in range(min_ngram, max_ngram+1):\n",
    "        n_grams += ngrams(tokens, gram_len)\n",
    "    # print(\"n_grams \",n_grams)\n",
    "    # print(\"===================\")\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data...\")\n",
    "df = pd.read_csv('data/data.csv')\n",
    "\n",
    "title = list(df.title.apply(str))\n",
    "short_desc = list(df.short_description.apply(str))\n",
    "long_desc = list(df.long_description.apply(str))\n",
    "\n",
    "labels = [str(x).strip().split('__') for x in df.manual_curation_value]\n",
    "# print(\"df head \",df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "k = 0\n",
    "for i in range(len(title)):\n",
    "    corpus = (title[i] + \" \" + short_desc[i] + \" \" + long_desc[i]).encode(\"ascii\", \"ignore\")\n",
    "    corpus = corpus.decode()\n",
    "    for j in corpus.split():\n",
    "        if j not in vocab:\n",
    "            vocab[j] = k\n",
    "            k+=1\n",
    "            \n",
    "vocab['UNK'] = k+1\n",
    "vocab['PAD'] = k+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentences = []        \n",
    "# train_labels = []\n",
    "\n",
    "# for i in range(len(title)):\n",
    "#     sentence = (title[i] + \" \" + short_desc[i] + \" \" + long_desc[i]).encode(\"ascii\", \"ignore\")\n",
    "#     sentence = sentence.decode()\n",
    "#     #replace each token by its index if it is in vocab\n",
    "#     #else use index of UNK\n",
    "#     s = [vocab[token] if token in vocab \n",
    "#         else vocab['UNK']\n",
    "#         for token in sentence.split(' ')]\n",
    "#     train_sentences.append(s)\n",
    "\n",
    "# # with open(train_labels_file) as f:\n",
    "# #     for sentence in f.read().splitlines():\n",
    "# #         #replace each label by its index\n",
    "# #         l = [tag_map[label] for label in sentence.split(' ')]\n",
    "# #         train_labels.append(l)\n",
    "        \n",
    "tag_map = {'B':0,'I':1,'O':2,'E':3,'S':4}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [tag_map[label] for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [vocab[token] if token in vocab else vocab['UNK'] for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BIOE tags...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating BIOE tags...\")\n",
    "output_words, output_chars = [], []\n",
    "train_labels,train_sentences,temp_labels = [],[],[]\n",
    "\n",
    "for i in range(len(title)):\n",
    "    corpus = (title[i] + \" \" + short_desc[i] + \" \" + long_desc[i]).encode(\"ascii\", \"ignore\")\n",
    "    corpus = corpus.decode()\n",
    "    label = labels[i]\n",
    "    \n",
    "    word_tokens = get_tokens(corpus, min_ngram=1, max_ngram=1)\n",
    "    char_tokens = get_tokens(corpus, min_ngram=1, max_ngram=1, is_char=True)\n",
    "    # print(\"word_tokens \",word_tokens,\" char_tokens \",char_tokens)\n",
    "    \n",
    "    word_tokens = [w for w in word_tokens if len(w) > 0]\n",
    "    char_tokens = [w for w in char_tokens if len(w) > 0]\n",
    "    \n",
    "    s = [vocab[token] if token in vocab \n",
    "        else vocab['UNK']\n",
    "        for token in word_tokens]\n",
    "    train_sentences.append(s)\n",
    "    \n",
    "    word_pos_tags = [y for x, y in nltk.pos_tag(word_tokens)]\n",
    "    char_pos_tags = ['UNK']*len(char_tokens)\n",
    "    # print(\"word_pos_tags \",word_pos_tags,\" char_pos_tags \",char_pos_tags)\n",
    "    \n",
    "    word_ner_tags = ['O']*len(word_tokens)\n",
    "    char_ner_tags = ['O']*len(char_tokens)\n",
    "    \n",
    "    word_tokens_lc = [w.lower() for w in word_tokens]\n",
    "    char_tokens_lc = [w.lower() for w in char_tokens]\n",
    "    l2,temp = [],[]\n",
    "    for j in range(len(label)):\n",
    "        label[j] = label[j].encode(\"ascii\", \"ignore\")\n",
    "        label[j] = label[j].decode()\n",
    "        gpt_word_tokens = get_tokens(label[j], min_ngram=1, max_ngram=1)\n",
    "        gpt_char_tokens = get_tokens(label[j], min_ngram=1, max_ngram=1, is_char=True)\n",
    "        # print(\"gpt_word_tokens \",gpt_word_tokens,\" gpt_char_tokens \",gpt_char_tokens)\n",
    "\n",
    "        gpt_word_tokens = [w for w in gpt_word_tokens if len(w) > 0]\n",
    "        gpt_char_tokens = [w for w in gpt_char_tokens if len(w) > 0]\n",
    "        \n",
    "        gpt_word_tokens_lc = [w.lower() for w in gpt_word_tokens]\n",
    "        gpt_char_tokens_lc = [w.lower() for w in gpt_char_tokens]\n",
    "        \n",
    "        n1, n2 = len(gpt_word_tokens), len(gpt_char_tokens)\n",
    "            \n",
    "        for k in range(len(word_tokens)-n1+1):\n",
    "            if (word_tokens[k:k+n1] == gpt_word_tokens or word_tokens_lc[k:k+n1] == gpt_word_tokens_lc) and word_ner_tags[k][0] == 'O':\n",
    "                if n1 == 1:\n",
    "                    word_ner_tags[k] = 'S'\n",
    "                elif n1 == 2:\n",
    "                    word_ner_tags[k] = 'B'\n",
    "                    word_ner_tags[k+1] = 'E'\n",
    "                else:\n",
    "                    word_ner_tags[k] = 'B'\n",
    "                    word_ner_tags[k+n1-1] = 'E'\n",
    "                    word_ner_tags[k+1:k+n1-1] = ['I']*(n1-2)\n",
    "                    \n",
    "        for k in range(len(char_tokens)-n2+1):\n",
    "            if (char_tokens[k:k+n2] == gpt_char_tokens or char_tokens_lc[k:k+n2] == gpt_char_tokens_lc) and char_ner_tags[k][0] == 'O':\n",
    "                if n2 == 1:\n",
    "                    char_ner_tags[k] = 'S'\n",
    "                elif n2 == 2:\n",
    "                    char_ner_tags[k] = 'B'\n",
    "                    char_ner_tags[k+1] = 'E'\n",
    "                else:\n",
    "                    char_ner_tags[k] = 'B'\n",
    "                    char_ner_tags[k+n2-1] = 'E'\n",
    "                    char_ner_tags[k+1:k+n2-1] = ['I']*(n2-2)\n",
    "              \n",
    "\n",
    "        for l in word_ner_tags:\n",
    "            l2.append(tag_map[l])\n",
    "#         print(\"corpus is \",corpus)\n",
    "#         print(\"label is \",label[j])\n",
    "#         print(\"tags are \",l2)\n",
    "#         print(\"word_ner_tags \",word_ner_tags)\n",
    "#         print(\"char_ner_tags \",char_ner_tags)\n",
    "        temp.append(label[j])\n",
    "    train_labels.append([l2[0]])\n",
    "    temp_labels.append(temp)\n",
    "        \n",
    "    \n",
    "    q_words = zip(word_tokens, word_pos_tags, word_ner_tags)\n",
    "    q_chars = zip(char_tokens, char_pos_tags, char_ner_tags)\n",
    "#     print(\"q_words \",list(q_words),\" q_chars \",list(q_chars))\n",
    "\n",
    "    output_words.append(list(q_words))\n",
    "    output_chars.append(list(q_chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train test split...\n",
      "Creating train test data...\n",
      "Building word model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating train test split...\")\n",
    "train_indices, valid_indices = train_test_split(range(len(output_words)), test_size=0.2, random_state=0)\n",
    "\n",
    "train_sents_words = [output_words[x] for x in train_indices if labels[x][0] != 'nan']\n",
    "valid_sents_words = [output_words[x] for x in valid_indices]\n",
    "\n",
    "train_sents_chars = [output_chars[x] for x in train_indices if labels[x][0] != 'nan']\n",
    "valid_sents_chars = [output_chars[x] for x in valid_indices]\n",
    "\n",
    "print(\"Creating train test data...\")\n",
    "# print(\"train_sents_words \",train_sents_words[5])\n",
    "\n",
    "X_train_words = [sent2tokens(s) for s in train_sents_words]\n",
    "y_train_words = [sent2labels(s) for s in train_sents_words]\n",
    "\n",
    "# print(\"X_train_words \",X_train_words[5],\" y_train_words \",y_train_words[5])\n",
    "X_train_chars = [sent2features(s) for s in train_sents_chars]\n",
    "y_train_chars = [sent2labels(s) for s in train_sents_chars]\n",
    "\n",
    "X_valid_words = [sent2tokens(s) for s in valid_sents_words]\n",
    "y_valid_words = [sent2labels(s) for s in valid_sents_words]\n",
    "\n",
    "X_valid_chars = [sent2features(s) for s in valid_sents_chars]\n",
    "y_valid_chars = [sent2labels(s) for s in valid_sents_chars]\n",
    "\n",
    "print(\"Building word model...\")\n",
    "# word_crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100, all_possible_transitions=True)\n",
    "# word_crf.fit(X_train_words, y_train_words)\n",
    "\n",
    "# print(\"Building char model...\")\n",
    "# char_crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100, all_possible_transitions=True)\n",
    "# char_crf.fit(X_train_chars, y_train_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = X_train_words+X_valid_words\n",
    "full_labels = y_train_words+y_valid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#compute length of longest sentence in batch\n",
    "batch_max_len = max([len(s) for s in full_text])\n",
    "\n",
    "# train_sents_words = [train_sentences[x] for x in train_indices if train_labels[x][0] != 'nan']\n",
    "# valid_sents_words = [train_sentences[x] for x in valid_indices]\n",
    "\n",
    "# train_labels_final = [train_labels[x] for x in train_indices if train_labels[x][0] != 'nan']\n",
    "# valid_labels = [train_labels[x] for x in valid_indices]\n",
    "# batch_max_len_char = max([len(s) for s in X_train_chars])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#prepare a numpy array with the data, initializing the data with 'PAD' \n",
    "#and all labels with 2; initializing labels to 2 differentiates tokens \n",
    "#with tags from 'PAD' tokens\n",
    "batch_data = vocab['PAD']*np.ones((len(full_text), batch_max_len))\n",
    "batch_labels = 2*np.ones((len(full_text), batch_max_len))\n",
    "\n",
    "# batch_data_char = 1*np.ones((len(X_train_chars), batch_max_len_char))\n",
    "# batch_labels_char = -1*np.ones((len(X_train_chars), batch_max_len_char))\n",
    "\n",
    "#copy the data to the numpy array\n",
    "for j in range(len(full_text)):\n",
    "    cur_len = len(full_text[j])\n",
    "#     print(\"X_train_words[j] \",X_train_words[j],\" batch_data[j][:cur_len] \",batch_data[j][:cur_len])\n",
    "    batch_data[j][:cur_len] = full_text[j]\n",
    "#     print(\"train_labels[j] \",len(train_labels[j]),\" len train_sentences[j] \",len(train_sentences[j]),\" temp_labels \",temp_labels[j])\n",
    "    batch_labels[j][:cur_len] = full_labels[j]\n",
    "    \n",
    "# for j in range(len(X_train_chars)):\n",
    "#     cur_len = len(X_train_chars[j])\n",
    "#     batch_data_char[j][:cur_len] = X_train_chars[j]\n",
    "#     batch_labels_char[j][:cur_len] = y_train_chars[j]\n",
    "\n",
    "#since all data are indices, we convert them to torch LongTensors\n",
    "batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "# batch_data_char, batch_labels_char = torch.LongTensor(batch_data_char), torch.LongTensor(batch_labels_char)\n",
    "\n",
    "#convert Tensors to Variables\n",
    "batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "# batch_data_char, batch_labels_char = Variable(batch_data_char), Variable(batch_labels_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Net, self).__init__()\n",
    "        #maps each token to an embedding_dim vector\n",
    "        self.embedding = nn.Embedding(params['vocab_size']+1, params['embedding_dim'])\n",
    "\n",
    "        #the LSTM takens embedded sentence\n",
    "        self.lstm = nn.LSTM(params['embedding_dim'], params['lstm_hidden_dim'], batch_first=True)\n",
    "\n",
    "        #fc layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(params['lstm_hidden_dim'], params['number_of_tags'])\n",
    "    \n",
    "    def forward(self, s):\n",
    "        #apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim\n",
    "        \n",
    "        #run the LSTM along the sentences of length batch_max_len\n",
    "        s, _ = self.lstm(s)     # dim: batch_size x batch_max_len x lstm_hidden_dim                \n",
    "        #reshape the Variable so that each row contains one token\n",
    "        s = s.reshape(s.shape[0]*s.shape[1], s.shape[2])  # dim: batch_size*batch_max_len x lstm_hidden_dim\n",
    "        #apply the fully connected layer and obtain the output for each token\n",
    "        s = self.fc(s)          # dim: batch_size*batch_max_len x num_tags\n",
    "\n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*batch_max_len x num_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(outputs, labels):\n",
    "#     #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "#     labels = labels.view(-1)  \n",
    "\n",
    "#     #mask out 'PAD' tokens\n",
    "#     mask = (labels >= 0).float()\n",
    "\n",
    "#     #the number of tokens is the sum of elements in mask\n",
    "#     num_tokens = int(torch.sum(mask).item())\n",
    "\n",
    "#     #pick the values corresponding to labels and multiply by mask\n",
    "#     outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
    "\n",
    "#     #cross entropy loss for all non 'PAD' tokens\n",
    "#     return -torch.sum(outputs)/num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self):\n",
    "    # import and initialize dataset\n",
    "    self.source = np.array(batch_data, dtype = int)\n",
    "    self.target = np.array(batch_labels, dtype = int)\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    # get item by index\n",
    "    return self.source[idx], self.target[idx]\n",
    "  \n",
    "  def __len__(self):\n",
    "    # returns length of data\n",
    "    return len(self.source)\n",
    "\n",
    "dataset = NMTDataset()\n",
    "NUM_INSTANCES = len(dataset)\n",
    "TEST_RATIO = 0.2\n",
    "TEST_SIZE = int(NUM_INSTANCES * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:57<00:00,  5.88s/it]\n"
     ]
    }
   ],
   "source": [
    "#train_data contains train_sentences and train_labels\n",
    "#params contains batch_size\n",
    "# dataset = (train_sentences, train_labels)\n",
    "params = {'vocab_size':len(vocab),'embedding_dim':50,'lstm_hidden_dim':32,'number_of_tags':len(tag_map)}\n",
    "\n",
    "model = Net(params)\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "\n",
    "indices = list(range(NUM_INSTANCES))\n",
    "test_idx = np.random.choice(indices, size = TEST_SIZE, replace = False)\n",
    "train_idx = list(set(indices) - set(test_idx))\n",
    "train_sampler, test_sampler = SubsetRandomSampler(train_idx), SubsetRandomSampler(test_idx)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size = 16, sampler = train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size = 16, sampler = test_sampler)\n",
    "\n",
    "num_training_steps = 20\n",
    "loss_trace,eval_loss = [],[]\n",
    "for _ in tqdm(range(num_training_steps)):\n",
    "    current_loss,epoch_loss = 0,0\n",
    "    model.train()\n",
    "    for i,(x,y) in enumerate(train_loader):\n",
    "        #pass through model, perform backpropagation and updates\n",
    "        outputs = model(x)\n",
    "        output_dim = outputs.shape[-1]\n",
    "#         print(\"initially outputs.shape \",outputs.shape ,\" y \",y.shape,\" y[1] \",y[1])\n",
    "        outputs = outputs.view(-1, output_dim)\n",
    "        y = y.view(-1)\n",
    "#         print(\"outputs.shape \",outputs.shape ,\" y \",y.shape,\" y[1] \",y[1])\n",
    "        loss = loss_fn(outputs,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss += loss.item()\n",
    "    loss_trace.append(current_loss)\n",
    "\n",
    "    model.eval()\n",
    "    for i, (x,y) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "#             x, y  = x.to(DEVICE), y.to(DEVICE)\n",
    "            outputs = model(x) #turn off teacher forcing\n",
    "            output_dim = outputs.shape[-1]\n",
    "\n",
    "            outputs = outputs.view(-1, output_dim)\n",
    "            y = y.view(-1)\n",
    "\n",
    "            loss = loss_fn(outputs, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    eval_loss.append(epoch_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_trace)\n",
    "plt.plot(eval_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions,orig_ans = [],[]\n",
    "for i, (x,y) in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x) #turn off teacher forcing\n",
    "        orig_ans.append(y)\n",
    "        predictions.append(outputs.max(1)[1])\n",
    "#         for output in outputs:\n",
    "#             _, indices = output.max(-1)\n",
    "#             predictions.append(indices.detach().cpu().numpy())\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
