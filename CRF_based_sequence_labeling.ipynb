{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn_crfsuite, re\n",
    "import numpy as np\n",
    "import importlib, os\n",
    "import logging, math\n",
    "import json, nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens, gram_len=2):\n",
    "    out = []\n",
    "    for i in range(len(tokens)-gram_len+1):\n",
    "        new_token = ' '.join(tokens[i:(i+gram_len)])\n",
    "        out.append(new_token.strip())\n",
    "    return out\n",
    "\n",
    "def clean_tokens(tokens, to_replace='[^\\w ]+'):\n",
    "    tokens = [re.sub(to_replace, ' ', token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def tokenize(mystr, is_char=False):\n",
    "    return mystr.split() if is_char is False else list(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentence, min_ngram=1, max_ngram=1, to_replace='[^\\w ]+', is_char=False):\n",
    "    # print(\"============================\")\n",
    "    # print(\"sentence before \",sentence)\n",
    "    if is_char is False:\n",
    "        sentence = re.sub('<[^<]+?>', ' ', sentence)\n",
    "        sentence = re.sub(to_replace, '', sentence)\n",
    "    # print(\"sentence after \",sentence)\n",
    "    tokens = clean_tokens(tokenize(sentence), to_replace) if is_char is False else tokenize(sentence, is_char)\n",
    "    # print(\"tokens before \",tokens)\n",
    "    tokens = [token.strip() for token in tokens] if is_char is False else [token for token in tokens]\n",
    "    # print(\"tokens after \",tokens)\n",
    "\n",
    "    n_grams = []\n",
    "    for gram_len in range(min_ngram, max_ngram+1):\n",
    "        n_grams += ngrams(tokens, gram_len)\n",
    "    # print(\"n_grams \",n_grams)\n",
    "    # print(\"===================\")\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading data...\")\n",
    "df = pd.read_csv('data/data.csv')\n",
    "\n",
    "text = list(df.text.apply(str))\n",
    "\n",
    "labels = [str(x).strip().split('__') for x in df.raw_label]\n",
    "print(\"df head \",df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating BIOE tags...\")\n",
    "output_words, output_chars = [], []\n",
    "for i in range(len(text)):\n",
    "    corpus = text[i].encode(\"ascii\", \"ignore\")\n",
    "    corpus = corpus.decode()\n",
    "    label = labels[i]\n",
    "    \n",
    "    word_tokens = get_tokens(corpus, min_ngram=1, max_ngram=1)\n",
    "    char_tokens = get_tokens(corpus, min_ngram=1, max_ngram=1, is_char=True)\n",
    "    # print(\"word_tokens \",word_tokens,\" char_tokens \",char_tokens)\n",
    "    \n",
    "    word_tokens = [w for w in word_tokens if len(w) > 0]\n",
    "    char_tokens = [w for w in char_tokens if len(w) > 0]\n",
    "    \n",
    "    word_pos_tags = [y for x, y in nltk.pos_tag(word_tokens)]\n",
    "    char_pos_tags = ['UNK']*len(char_tokens)\n",
    "    # print(\"word_pos_tags \",word_pos_tags,\" char_pos_tags \",char_pos_tags)\n",
    "    \n",
    "    word_ner_tags = ['O']*len(word_tokens)\n",
    "    char_ner_tags = ['O']*len(char_tokens)\n",
    "    \n",
    "    word_tokens_lc = [w.lower() for w in word_tokens]\n",
    "    char_tokens_lc = [w.lower() for w in char_tokens]\n",
    "    \n",
    "    for j in range(len(label)):\n",
    "        label[j] = label[j].encode(\"ascii\", \"ignore\")\n",
    "        label[j] = label[j].decode()\n",
    "        gpt_word_tokens = get_tokens(label[j], min_ngram=1, max_ngram=1)\n",
    "        gpt_char_tokens = get_tokens(label[j], min_ngram=1, max_ngram=1, is_char=True)\n",
    "        # print(\"gpt_word_tokens \",gpt_word_tokens,\" gpt_char_tokens \",gpt_char_tokens)\n",
    "\n",
    "        gpt_word_tokens = [w for w in gpt_word_tokens if len(w) > 0]\n",
    "        gpt_char_tokens = [w for w in gpt_char_tokens if len(w) > 0]\n",
    "        \n",
    "        gpt_word_tokens_lc = [w.lower() for w in gpt_word_tokens]\n",
    "        gpt_char_tokens_lc = [w.lower() for w in gpt_char_tokens]\n",
    "        \n",
    "        n1, n2 = len(gpt_word_tokens), len(gpt_char_tokens)\n",
    "            \n",
    "        for k in range(len(word_tokens)-n1+1):\n",
    "            if (word_tokens[k:k+n1] == gpt_word_tokens or word_tokens_lc[k:k+n1] == gpt_word_tokens_lc) and word_ner_tags[k][0] == 'O':\n",
    "                if n1 == 1:\n",
    "                    word_ner_tags[k] = 'S'\n",
    "                elif n1 == 2:\n",
    "                    word_ner_tags[k] = 'B'\n",
    "                    word_ner_tags[k+1] = 'E'\n",
    "                else:\n",
    "                    word_ner_tags[k] = 'B'\n",
    "                    word_ner_tags[k+n1-1] = 'E'\n",
    "                    word_ner_tags[k+1:k+n1-1] = ['I']*(n1-2)\n",
    "                    \n",
    "        for k in range(len(char_tokens)-n2+1):\n",
    "            if (char_tokens[k:k+n2] == gpt_char_tokens or char_tokens_lc[k:k+n2] == gpt_char_tokens_lc) and char_ner_tags[k][0] == 'O':\n",
    "                if n2 == 1:\n",
    "                    char_ner_tags[k] = 'S'\n",
    "                elif n2 == 2:\n",
    "                    char_ner_tags[k] = 'B'\n",
    "                    char_ner_tags[k+1] = 'E'\n",
    "                else:\n",
    "                    char_ner_tags[k] = 'B'\n",
    "                    char_ner_tags[k+n2-1] = 'E'\n",
    "                    char_ner_tags[k+1:k+n2-1] = ['I']*(n2-2)\n",
    "    \n",
    "    q_words = zip(word_tokens, word_pos_tags, word_ner_tags)\n",
    "    q_chars = zip(char_tokens, char_pos_tags, char_ner_tags)\n",
    "    # print(\"q_words \",list(q_words),\" q_chars \",list(q_chars))\n",
    "\n",
    "    output_words.append(list(q_words))\n",
    "    output_chars.append(list(q_chars))\n",
    "\n",
    "print(\"Creating train test split...\")\n",
    "train_indices, valid_indices = train_test_split(range(len(output_words)), test_size=0.2, random_state=0)\n",
    "\n",
    "train_sents_words = [output_words[x] for x in train_indices if labels[x][0] != 'nan']\n",
    "valid_sents_words = [output_words[x] for x in valid_indices]\n",
    "\n",
    "train_sents_chars = [output_chars[x] for x in train_indices if labels[x][0] != 'nan']\n",
    "valid_sents_chars = [output_chars[x] for x in valid_indices]\n",
    "\n",
    "print(\"Creating train test data...\")\n",
    "# print(\"train_sents_words \",train_sents_words[5])\n",
    "\n",
    "X_train_words = [sent2features(s) for s in train_sents_words]\n",
    "y_train_words = [sent2labels(s) for s in train_sents_words]\n",
    "\n",
    "# print(\"X_train_words \",X_train_words[5],\" y_train_words \",y_train_words[5])\n",
    "X_train_chars = [sent2features(s) for s in train_sents_chars]\n",
    "y_train_chars = [sent2labels(s) for s in train_sents_chars]\n",
    "\n",
    "X_valid_words = [sent2features(s) for s in valid_sents_words]\n",
    "y_valid_words = [sent2labels(s) for s in valid_sents_words]\n",
    "\n",
    "X_valid_chars = [sent2features(s) for s in valid_sents_chars]\n",
    "y_valid_chars = [sent2labels(s) for s in valid_sents_chars]\n",
    "\n",
    "print(\"Building word model...\")\n",
    "word_crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100, all_possible_transitions=True)\n",
    "word_crf.fit(X_train_words, y_train_words)\n",
    "\n",
    "print(\"Building char model...\")\n",
    "char_crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100, all_possible_transitions=True)\n",
    "char_crf.fit(X_train_chars, y_train_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Doing predictions...\")\n",
    "predictions_words = word_crf.predict(X_valid_words)\n",
    "predictions_chars = char_crf.predict(X_valid_chars)\n",
    "\n",
    "print(\"Doing word level predictions...\")\n",
    "out_vals_words = []\n",
    "\n",
    "for i in range(len(predictions_words)):\n",
    "    curr_val, all_curr_val = [], []\n",
    "    # print(\"predictions_words[i] \",predictions_words[i][0])\n",
    "    for j in range(len(predictions_words[i])):\n",
    "        if predictions_words[i][j][0] == 'B' or predictions_words[i][j][0] == 'S':\n",
    "            if len(curr_val) > 0:\n",
    "                all_curr_val.append(' '.join(curr_val))\n",
    "            curr_val = [valid_sents_words[i][j][0]]\n",
    "            \n",
    "        elif predictions_words[i][j][0] == 'I' or predictions_words[i][j][0] == 'E':\n",
    "            curr_val += [valid_sents_words[i][j][0]]\n",
    "    \n",
    "    if len(curr_val) > 0:\n",
    "        all_curr_val.append(' '.join(curr_val))\n",
    "    \n",
    "    out_vals_words.append(all_curr_val)\n",
    "\n",
    "print(\"Doing char level predictions...\")\n",
    "out_vals_chars = []\n",
    "\n",
    "for i in range(len(predictions_chars)):\n",
    "    curr_val, all_curr_val = [], []\n",
    "    \n",
    "    for j in range(len(predictions_chars[i])):\n",
    "        # print(\"predictions_chars[i] \",predictions_chars[i][0])\n",
    "        if predictions_chars[i][j][0] == 'B' or predictions_chars[i][j][0] == 'S':\n",
    "            if len(curr_val) > 0:\n",
    "                all_curr_val.append(''.join(curr_val))\n",
    "            curr_val = [valid_sents_chars[i][j][0]]\n",
    "            \n",
    "        elif predictions_chars[i][j][0] == 'I' or predictions_chars[i][j][0] == 'E':\n",
    "            curr_val += [valid_sents_chars[i][j][0]]\n",
    "    \n",
    "    if len(curr_val) > 0:\n",
    "        all_curr_val.append(''.join(curr_val))\n",
    "    \n",
    "    out_vals_chars.append(all_curr_val)\n",
    "\n",
    "print(\"Merging predictions...\")\n",
    "out_vals = []\n",
    "for i in range(len(out_vals_words)):\n",
    "    if out_vals_words[i] == 'None':\n",
    "        out_vals.append(out_vals_chars[i])\n",
    "    else:\n",
    "        out_vals.append(out_vals_words[i])\n",
    "\n",
    "pred_labels = []\n",
    "for i in out_vals:\n",
    "    if len(i)>0:\n",
    "        pred_labels.append(i[0])\n",
    "    else:\n",
    "        pred_labels.append('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_labels = [str(x).strip().split('__') for x in df.raw_label]\n",
    "norm_labels = [norm_labels[x] for x in valid_indices]\n",
    "true_labels = [x[0].encode(\"ascii\", \"ignore\").decode() for x in norm_labels]\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
